
\documentclass{article}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{palatino}
\usepackage{bm}
\geometry{verbose,letterpaper,tmargin=1in,bmargin=.75in,lmargin=.75in,rmargin=1in}
\date{\today}
\newtheorem{thm}{Theorem}[subsection]
\newtheorem{deff}[thm]{Definition}
\newtheorem{rmk}[thm]{Remark}
% \newtheorem{prf}[thm]{Proof}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{emp}[thm]{Example}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{pps}[thm]{Proposition}

\newcommand{\polya}{P\'{o}lya}

\begin{document}
\today

\section{Multivariate Bayesian Quantile Regression with \polya{} Tree}

\subsection{Multivariate \polya{} Tree}

Due to the difficulty in definition of quantile in multivariate case
and diversity of partition methods, there are only few literatures
about \polya{} tree priors for multivariate data. Paddock (1999, 2002)
extended univariate \polya{} tree (Lavine 1992, 1994) to multivariate
case based on a q-dimensional hypercube. Partitions are constructed
through a series of binary recursive perpendicular splits of each axis
of the hypercube. Hanson (2006) proposed a q-dimensional
location-scale mixture of finite \polya{} tree which is a direct
generalization of the univariate finite location-scale \polya{}
tree. Jara (2009) extended the multivariate \polya{} tree prior based
on Hanson (2006) with an additional parameter: directional orthogonal
matrix. 

Based on Hanson (2006) and Jara (2009), we briefly introduce
multivariate \polya{} tree prior as follows: Let $E=\{0,1\}$,
$E^m=\{0,1\}^m$ be m-fold product of $E$, and $\pi_j = \left\{
  B_{\epsilon_{11}\cdots
    \epsilon_{1j};\cdots;\epsilon_{q1}\cdots\epsilon_{qj}};
  \epsilon_{ij}\in E \right\}$ be a level $j$ partition set of
$\Omega$ such that $\pi_{j+1}$ are the $2^q$ finer partitions of
$\pi_j$. 

\begin{deff}[Multivariate \polya{} Tree Distribution]
A q-dimensional random probability measure $G$ is said to have a
multivariate \polya{} tree distribution with parameters $(\Pi,
\mathcal{A})$, if there exists nonnegative numbers
$\mathcal{A}=\left\{ \alpha_{\varepsilon_1;\cdots;\varepsilon_q} ;
  \varepsilon_1, \cdots, \varepsilon_q \in E^j, j=1, \ldots \right\}$
(note: $\varepsilon_i$ indicates which position the bin takes in level
$j$ with respect to $i^{th}$ dimension) and random vectors
$\mathcal{Y} = \left\{ Y_{\varepsilon_1;\cdots;\varepsilon_q} ;
  \varepsilon_1, \cdots, \varepsilon_q \in E^j, j=1, \cdots \right\}$,
such that the following hold: 
\begin{enumerate}
\item All of the random vectors in $\mathcal{Y}$ are independent,
\item For $j=1, \ldots$ and for all $\varepsilon_1, \cdots,
  \varepsilon_q \in E^j$, $\bm{Y}_{\varepsilon_1;\cdots;\varepsilon_q}
  \sim \mathrm{Dirichlet}\left( \bm{\alpha}_{\varepsilon_1; \cdots;
      \varepsilon_q} \right)$, where
  $\bm{Y}_{\varepsilon_1;\cdots;\varepsilon_q} = \left\{
    y_{\varepsilon_1\epsilon_1; \cdots; \varepsilon_q\epsilon_q};
    \epsilon_1, \cdots, \epsilon_q \in E \right\}$ and $\bm{\alpha}_{\varepsilon_1; \cdots;
      \varepsilon_q} = \left\{ \alpha_{\varepsilon_1\epsilon_1;
        \cdots; \varepsilon_q\epsilon_q}; \epsilon_1, \cdots,
      \epsilon_q \in E \right\}$,
\item For every $j=1,2, \cdots$, 
\begin{displaymath}
G(B_{\epsilon_{11},\cdots,
  \epsilon_{1j};\cdots;\epsilon_{q1}\cdots\epsilon_{qj}}) =
\prod_{l=1}^j Y_{\epsilon_{11}\cdots \epsilon_{1l}; \cdots ; \epsilon_{q1}\cdots\epsilon_{ql}}.
\end{displaymath}
\end{enumerate}
\end{deff}

Similar to univariate \polya{} tree, the canonical way of partition
construction is based on reverting cdf of the centering
distribution. First, suppose $G_0$ is a univariate cdf and its
corresponding pdf is $g_0(\omega)$. Define $g_0(\bm{\omega}=(\omega_1,
\ldots, \omega_q)) = \prod_{i=1}^q g_0(\omega_i)$. Denote
$\bm{\theta}= (\bm{\mu}_q, \bm{\Sigma}_{q\times q}$ as location-scale
parameters, then a family of location-scale baseline measures for
multivariate \polya{} tree have the following pdf forms
$g_{\bm{\theta}} ( \bm{\omega}) = |\bm{\Sigma}|^{-1/2} g_0 (
\bm{\Sigma}^{-1/2} (\bm{\omega} - \bm{\mu}) ) $. 

For baseline measure $g_0(\bm{\omega})$, the partition $\Pi_0^j$ of
$\mathrm{R}^q$ are obtained from cross-products of corresponding
univariate partition sets. Denote 
$$B_0( \epsilon_{11}\cdots
    \epsilon_{1j};\cdots;\epsilon_{q1}\cdots\epsilon_{qj}) =
    B_0(e_j(k_1)) \times B_0(e_j(k_2)) \times \cdots \times
    B_0(e_j(k_q)),$$
where $B_0(e_j(k))= \left( G_0^{-1}((k-1)2^{-j}), G_0^{-1}(k2^{-j})
\right)$. 

Denote $\bm{e}_j(\bm{k})= e_j(k_1); \cdots;e_j(k_q)$, where $\bm{k}=
(k_1, \ldots, k_q)$, then partitions $\Pi_{\theta}^j$ from
location-scale baseline measure family $G_{\bm{\theta}}$ or
$g_{\bm{\theta}}(\bm{\omega})$ are defined as 
\begin{displaymath}
B_{\bm{\theta}}(\bm{e}_j(\bm{k})) = \left\{ \bm{\mu} +
  \bm{\Sigma}^{1/2} \bm{y}; \bm{y} \in B_0(\bm{e}_j(\bm{k})) \right\}.
\end{displaymath}

Jara (2009) pointed out that the direction of the sets in Hanson
(2006) is complete defined by the decomposition of the covariance
matrix, the unique symmetric square root. Instead, he introduced
another orthogonal matrix as additional parameter to control the
direction of the sets. 

Suppose $\bm{\Sigma} = \bm{T'T}$, where $\bm{T}$ is the unique upper
triangular Cholesky matrix, then for any orthogonal matrix $\bm{O}$,
let $\bm{U=OT}$, then $\bm{U}$ is also a square root of
$\bm{\Sigma}$. Therefore, if we put a prior for $\bm{O}$ on the space
of all $q \times q$ orthogonal matrices, then we have a prior on all
possible square roots of $\bm{\Sigma}$, which control the direction of
the partition sets.

The uniqueness in Lemma 1 (Jara 2009) can show it is well defined. In
this way, the location-scale transformation induced partition sets $B_{\bm{\theta}}(\bm{e}_j(\bm{k})) = \left\{ \bm{\mu} +
  \bm{T'O'} \bm{z}; \bm{z} \in B_0(\bm{e}_j(\bm{k})) \right\}.$ The
Haar measure (Halmos 1950) provides an easy way to sample orthogonal
matrix $\bm{O}$ ``uniformly''. 

\subsection{Multivariate Regression with \polya{} Tree}
State some motivation

In order to address clustered or correlated data, we propose to model
multivariate errors directly, instead of adding random effects. We
assume each component of subject's multivariate response can be
affected by covariates respectively on its mean and variance,
therefore we propose a heterogeneous q-dimensional multivariate
regression model: 
\begin{displaymath}
\bm{Y}_i = \bm{X}_i \bm{B} + (\bm{X}_i\bm{\Gamma}) \circ \bm{E}_i,
\end{displaymath}
where $\bm{Y}_i = [y_{i1}, \ldots, y_{iq}]^T$, $\bm{X}_i= [x_{i1},
\cdots, x_{ip}]$, $\bm{E}_i =[\epsilon_{i1}, \cdots, \epsilon_{iq}]$, 

\begin{displaymath}
\bm{B}_{p\times q} = 
\begin{bmatrix}
\beta_{11}& \cdots & \beta_{1q} \\
\vdots & \ddots & \vdots \\
\beta_{p1} & \cdots & \beta_{pq}
\end{bmatrix}
\Gamma_{p\times q} = 
\begin{bmatrix}
\gamma_{11} & \cdots & \gamma_{1q} \\
\vdots & \ddots & \vdots \\
\gamma_{p1} & \cdots & \gamma_{pq}
\end{bmatrix}
\end{displaymath}

so suppose there $n$ subjects and $q$ dimensional responses for each
subject: 
\begin{displaymath}
\begin{bmatrix}
y_{11} &\cdots & y_{1q} \\
\vdots & \ddots & \vdots \\
y_{n1} & \cdots & y_{nq} 
\end{bmatrix}_{n\times q}
 = 
\begin{bmatrix}
x_{11} &\cdots & x_{1p} \\
\vdots & \ddots & \vdots \\
x_{n1} & \cdots & x_{np} 
\end{bmatrix}_{n\times p}
\begin{bmatrix}
\beta_{11} &\cdots & \beta_{1q} \\
\vdots & \ddots & \vdots \\
\beta_{p1} & \cdots & \beta_{pq} 
\end{bmatrix}_{p\times q}
+ 
\left( \begin{bmatrix}
x_{11} &\cdots & x_{1p} \\
\vdots & \ddots & \vdots \\
x_{n1} & \cdots & x_{np} 
\end{bmatrix}
\begin{bmatrix}
\gamma_{11} &\cdots & \gamma_{1q} \\
\vdots & \ddots & \vdots \\
\gamma_{p1} & \cdots & \gamma_{pq} 
\end{bmatrix}
 \right)
\circ
\begin{bmatrix}
\epsilon_{11} &\cdots & \epsilon_{1q} \\
\vdots & \ddots & \vdots \\
\epsilon_{n1} & \cdots & \epsilon_{nq} 
\end{bmatrix}_{n\times q}
\end{displaymath}
in which $'\circ'$ is Hadamard product, aka entrywise product. We
assign a multivariate \polya{} tree prior on the error:
\begin{align*}
\bm{E}_i = [\epsilon_{i1}, \cdots, \epsilon_{iq}]^T
 & \stackrel{\text{i.i.d}}{\sim} G_{\bm{\theta}} \\
G_{\bm{\theta}} | \bm{\mu, \Sigma, O} & \sim PT \left( \Pi^{\bm{\mu,
      \Sigma, O}}, \mathcal{A} \right).
\end{align*}
Furthermore, in order not to confound with $\bm{beta}$ estimates, we
set $\bm{\mu} = \bm{0}$ and medians for each component of
$G_{\bm{\theta}}$ are fixed at 0. As to heterogeneity parameters
$\gamma_{ij}$, for the same reason, we restrict $\gamma_{1j}=1$ and
for all $\bm{\gamma_{.j}}, \bm{x}_{i.}$, $\bm{x_{i.}\gamma_{.j}}>0$
for all $i,j$. 

\subsection{Demonstration}
Here we illustrate our approach in several examples. Suppose we have a
bunch of bivariate data ($q=2$) , and there are $n=100$ observation
subjects in each dataset. Details about four datasets are listed
below:
\begin{enumerate}
\item [M1:]
$\begin{pmatrix}
y_{i1}\\
y_{i2}
\end{pmatrix}
= 
\begin{pmatrix}
1 & x_{i1} & x_{i2}
\end{pmatrix}
\begin{pmatrix}
1& 1\\
1&1\\
1&1
\end{pmatrix}
+
\begin{pmatrix}
1 & x_{i1} & x_{i2}
\end{pmatrix}
\begin{pmatrix}
1 & 1\\
0 & 0\\
0 & 0
\end{pmatrix}
\circ \bm{\epsilon}_{1i}, \bm{\epsilon_{1i}} \sim \mathrm{N} \left( 
\begin{pmatrix}
0\\
0
\end{pmatrix}
, 
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
 \right)$.
\item [M2:]
$\begin{pmatrix}
y_{i1}\\
y_{i2}
\end{pmatrix}
= 
\begin{pmatrix}
1 & x_{i1}  &  x_{i2} 
\end{pmatrix}
\begin{pmatrix}
1& 1\\
2&1\\
3&1
\end{pmatrix}
+
\begin{pmatrix}
1 &  x_{i1}  &  x_{i2} 
\end{pmatrix}
\begin{pmatrix}
1 & 1\\
0 & 0\\
0 & 0
\end{pmatrix}
 \circ \bm{\epsilon}_{2i} ,  \bm{\epsilon_{2i}} \sim \mathrm{N} \left( 
\begin{pmatrix}
0\\
0
\end{pmatrix}
, 
\begin{pmatrix}
1 & 0.5 \\
0.5 & 1
\end{pmatrix}
 \right)$.
\item [M3:]
$\begin{pmatrix}
 y_{i1} \\
 y_{i2} 
\end{pmatrix}
= 
\begin{pmatrix}
1 &  x_{i1}  &  x_{i2} 
\end{pmatrix}
\begin{pmatrix}
1& 1\\
1&1\\
1&1
\end{pmatrix}
+
\begin{pmatrix}
1 &  x_{i1}  &  x_{i2} 
\end{pmatrix}
\begin{pmatrix}
1 & 1\\
0 & 0\\
0 & 0
\end{pmatrix}
 \circ \bm{\epsilon}_{3i}$ ,

 $ \bm{\epsilon_{3i}} \sim 0.5 \times \mathrm{N} \left( 
\begin{pmatrix}
-1.35\\
0.28
\end{pmatrix}
, 
\begin{pmatrix}
0.15 & 0.02 \\
0.02 & 0.04
\end{pmatrix}
 \right)
+ 
0.5 \times \mathrm{N} \left( 
\begin{pmatrix}
1.35\\
0.28
\end{pmatrix}
, 
\begin{pmatrix}
0.15 & 0.02 \\
0.02 & 0.04
\end{pmatrix}
 \right)
$.
\item [M4:]
$\begin{pmatrix}
 y_{i1} \\
 y_{i2} 
\end{pmatrix}
= 
\begin{pmatrix}
1 &  x_{i1}  &  x_{i2} 
\end{pmatrix}
\begin{pmatrix}
1& 1\\
2&1\\
3&1
\end{pmatrix}
+
\begin{pmatrix}
1 &  x_{i1}  &  x_{i2} 
\end{pmatrix}
\begin{pmatrix}
1 & 1\\
0 & -0.5\\
0 & 0.5
\end{pmatrix}
 \circ \bm{\epsilon}_{3i}$
\end{enumerate}
where $\epsilon_{3i}$ are coming from Jara (2009) illustration
section.  Model 1,2,3 (M1, M2, M3) considered homogeneous model with
no correlated normal error, correlated normal error and correlated
mixture of bivariate normal error respectively. Model 4 (M4) assigned
heterogeneity to mixture of bivariate normal error. Estimates from
mean of parameters posterior distributions are shown below:
\begin{table}[htbp]
\caption[]{\label{tab:multi-est} Posterior Estimates for Model 1-4}
\vspace{4mm}
\centering
\begin{tabular}[tb]{l|l|l|l}
Model &Term & Estimates & True \\
\hline
M1 & $\bm{B}$&
$\begin{pmatrix}
1.05 & 0.97 \\
1.04 & 0.96 \\
0.89 & 1.06 
\end{pmatrix}$
& 
$\begin{pmatrix}
1 & 1\\
1 & 1\\
1 & 1
\end{pmatrix}$ \\
& $\bm{\Gamma}$&
$\begin{pmatrix}
1.00 & 1.00 \\
0.02 & -0.20 \\
-0.04 & 0.14
\end{pmatrix}$
& 
$\begin{pmatrix}
1 & 1\\
0 & 0\\
0 & 0
\end{pmatrix} $\\
\hline
M2 & $\bm{B}$&
$\begin{pmatrix}
0.88 & 1.04 \\
1.96 & 1.21 \\
3.03 & 0.86 
\end{pmatrix}$
& 
$\begin{pmatrix}
1 & 1\\
2 & 1\\
3 & 1
\end{pmatrix}$ \\
& $\bm{\Gamma}$&
$\begin{pmatrix}
1.00 & 1.00 \\
0.09 & -0.02 \\
0.01 & 0.18
\end{pmatrix}$
& 
$\begin{pmatrix}
1 & 1\\
0 & 0\\
0 & 0
\end{pmatrix}$ \\
\hline
M3 & $\bm{B}$&
$\begin{pmatrix}
0.95 & 1.22 \\
1.02 & 0.97 \\
0.97 & 1.03 
\end{pmatrix}$
& 
$\begin{pmatrix}
1 & 1\\
1 & 1\\
1 & 1
\end{pmatrix}$ \\
& $\bm{\Gamma}$&
$\begin{pmatrix}
1.00 & 1.00 \\
0.04 & -0.07 \\
0.02 & 0.00
\end{pmatrix}$
& 
$\begin{pmatrix}
1 & 1\\
0 & 0\\
0 & 0
\end{pmatrix} $\\
\hline
M4 & $\bm{B}$&
$\begin{pmatrix}
0.98 & 1.24 \\
2.02 & 0.80 \\
2.94 & 1.17 
\end{pmatrix}$
& 
$\begin{pmatrix}
1 & 1\\
2 & 1\\
3 & 1
\end{pmatrix} $\\
& $\bm{\Gamma}$&
$\begin{pmatrix}
1.00 & 1.00 \\
0.03 & -0.55 \\
0.02 & 0.39
\end{pmatrix}$
& 
$\begin{pmatrix}
1 & 1\\
0 & -0.5\\
0 & 0.5
\end{pmatrix} $
\end{tabular}

\end{table}

\end{document}